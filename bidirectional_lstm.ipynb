{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Двунаправленная LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим текст для обучения из txt файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('american_psycho.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = re.sub(r'[^a-zA-Z\\s]', '', text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим токенизатор и словарь слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "\n",
    "word_dict = {}\n",
    "for word in tokens:\n",
    "    if word not in word_dict:\n",
    "        word_dict[word] = len(word_dict) + 1\n",
    "\n",
    "reverse_word_dict = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "sequences = [word_dict[word] for word in tokens]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим обучающие примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "sequence_length = 50\n",
    "\n",
    "for i in range(len(sequences) - sequence_length):\n",
    "    X.append(sequences[i:i + sequence_length])\n",
    "    y.append(sequences[i + sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y, num_classes=len(word_dict) + 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_dict) + 1, output_dim=embedding_dim, input_length=sequence_length))\n",
    "model.add(Bidirectional(LSTM(hidden_units)))\n",
    "model.add(Dense(len(word_dict) + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1097/1097 [==============================] - 512s 464ms/step - loss: 7.3474 - accuracy: 0.0463\n",
      "Epoch 2/20\n",
      "1097/1097 [==============================] - 532s 485ms/step - loss: 6.7270 - accuracy: 0.0783\n",
      "Epoch 3/20\n",
      "1097/1097 [==============================] - 563s 513ms/step - loss: 6.2927 - accuracy: 0.1014\n",
      "Epoch 4/20\n",
      "1097/1097 [==============================] - 541s 494ms/step - loss: 5.9455 - accuracy: 0.1183\n",
      "Epoch 5/20\n",
      "1097/1097 [==============================] - 537s 489ms/step - loss: 5.5805 - accuracy: 0.1333\n",
      "Epoch 6/20\n",
      "1097/1097 [==============================] - 567s 517ms/step - loss: 5.2255 - accuracy: 0.1493\n",
      "Epoch 7/20\n",
      "1097/1097 [==============================] - 557s 507ms/step - loss: 4.8330 - accuracy: 0.1679\n",
      "Epoch 8/20\n",
      "1097/1097 [==============================] - 560s 511ms/step - loss: 4.4043 - accuracy: 0.2051\n",
      "Epoch 9/20\n",
      "1097/1097 [==============================] - 527s 481ms/step - loss: 3.9774 - accuracy: 0.2601\n",
      "Epoch 10/20\n",
      "1097/1097 [==============================] - 514s 469ms/step - loss: 3.6072 - accuracy: 0.3094\n",
      "Epoch 11/20\n",
      "1097/1097 [==============================] - 531s 484ms/step - loss: 3.3062 - accuracy: 0.3521\n",
      "Epoch 12/20\n",
      "1097/1097 [==============================] - 550s 502ms/step - loss: 3.0558 - accuracy: 0.3888\n",
      "Epoch 13/20\n",
      "1097/1097 [==============================] - 543s 495ms/step - loss: 2.8359 - accuracy: 0.4243\n",
      "Epoch 14/20\n",
      "1097/1097 [==============================] - 543s 495ms/step - loss: 2.6434 - accuracy: 0.4565\n",
      "Epoch 15/20\n",
      "1097/1097 [==============================] - 543s 495ms/step - loss: 2.4685 - accuracy: 0.4866\n",
      "Epoch 16/20\n",
      "1097/1097 [==============================] - 529s 483ms/step - loss: 2.3089 - accuracy: 0.5147\n",
      "Epoch 17/20\n",
      "1097/1097 [==============================] - 524s 478ms/step - loss: 2.1624 - accuracy: 0.5408\n",
      "Epoch 18/20\n",
      "1097/1097 [==============================] - 519s 473ms/step - loss: 2.0268 - accuracy: 0.5671\n",
      "Epoch 19/20\n",
      "1097/1097 [==============================] - 525s 478ms/step - loss: 1.9032 - accuracy: 0.5905\n",
      "Epoch 20/20\n",
      "1097/1097 [==============================] - 521s 475ms/step - loss: 1.7877 - accuracy: 0.6136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18f80f2ed90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сгенерировать текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text):\n",
    "    seed_tokens = word_tokenize(seed_text)\n",
    "    seed_sequence = [word_dict[word] for word in seed_tokens]\n",
    "\n",
    "    for _ in range(50):\n",
    "        input_sequence = pad_sequences([seed_sequence], maxlen=sequence_length)\n",
    "        predicted_probs = model.predict(input_sequence)[0]\n",
    "        predicted_word_index = np.random.choice(np.argsort(predicted_probs)[-5:][::-1])\n",
    "        seed_sequence.append(predicted_word_index)\n",
    "        seed_sequence = seed_sequence[-sequence_length:]\n",
    "\n",
    "    return seed_text + ' ' + ' '.join([reverse_word_dict[index] for index in seed_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "My name is Patrick Bateman is the right commercial Evelyn has to talk Evelyn screams up Evelyn says Shes not paying some eerie muscles dont go back or the waiter begins Uh yeah Bateman Helga asks her confused cuts me to inspect a straight at a passing book But my god Evelyn says No Im\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'My name is Patrick Bateman'\n",
    "generated_text = generate_text(seed_text)\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
