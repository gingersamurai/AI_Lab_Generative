{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset, AUTOTUNE\n",
        "from tensorflow import keras\n",
        "from typing import Dict, Tuple\n",
        "import keras.layers as l\n",
        "from keras import models, callbacks, utils, losses"
      ],
      "metadata": {
        "id": "EvyxcS-gmU2R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ''\n",
        "with open('harry_potter.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "def get_features_target(seq: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    features = seq[:-1]\n",
        "    target = seq[1:]\n",
        "    return features, target\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "alphabet = np.array(sorted(set(text)))\n",
        "\n",
        "word_index = {char: i for i, char in enumerate(alphabet)}\n",
        "index_word = {i: char for i, char in enumerate(alphabet)}\n",
        "\n",
        "sequences = Dataset.from_tensor_slices(np.array([word_index[char] for char in text])).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = sequences.map(get_features_target)\n",
        "\n",
        "data = dataset.batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "data = data.prefetch(AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "h808pCammmwI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    l.Embedding(len(alphabet), BATCH_SIZE, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    l.SimpleRNN(512, return_sequences=True, stateful=True),\n",
        "    l.SimpleRNN(512, return_sequences=True, stateful=True),\n",
        "    l.Dense(len(alphabet))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss=losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "model.fit(data, epochs=20, verbose=1, steps_per_epoch= len(sequences) // BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH7Bjz_gEwU-",
        "outputId": "ce0de120-f740-48ed-9126-a6fa294673b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "5/5 [==============================] - 3s 359ms/step - loss: 5.4671 - accuracy: 0.0406\n",
            "Epoch 2/20\n",
            "5/5 [==============================] - 2s 353ms/step - loss: 4.0479 - accuracy: 0.1281\n",
            "Epoch 3/20\n",
            "5/5 [==============================] - 2s 345ms/step - loss: 3.4092 - accuracy: 0.1269\n",
            "Epoch 4/20\n",
            "5/5 [==============================] - 2s 345ms/step - loss: 3.3715 - accuracy: 0.1363\n",
            "Epoch 5/20\n",
            "5/5 [==============================] - 2s 343ms/step - loss: 3.3526 - accuracy: 0.1363\n",
            "Epoch 6/20\n",
            "5/5 [==============================] - 2s 341ms/step - loss: 3.3446 - accuracy: 0.1363\n",
            "Epoch 7/20\n",
            "5/5 [==============================] - 2s 348ms/step - loss: 3.3368 - accuracy: 0.1363\n",
            "Epoch 8/20\n",
            "5/5 [==============================] - 2s 345ms/step - loss: 3.3327 - accuracy: 0.1363\n",
            "Epoch 9/20\n",
            "5/5 [==============================] - 2s 342ms/step - loss: 3.3308 - accuracy: 0.1363\n",
            "Epoch 10/20\n",
            "5/5 [==============================] - 2s 354ms/step - loss: 3.3278 - accuracy: 0.1363\n",
            "Epoch 11/20\n",
            "5/5 [==============================] - 2s 343ms/step - loss: 3.3263 - accuracy: 0.1363\n",
            "Epoch 12/20\n",
            "5/5 [==============================] - 2s 348ms/step - loss: 3.3235 - accuracy: 0.1363\n",
            "Epoch 13/20\n",
            "5/5 [==============================] - 2s 342ms/step - loss: 3.3208 - accuracy: 0.1363\n",
            "Epoch 14/20\n",
            "5/5 [==============================] - 2s 341ms/step - loss: 3.3180 - accuracy: 0.1363\n",
            "Epoch 15/20\n",
            "5/5 [==============================] - 2s 351ms/step - loss: 3.3129 - accuracy: 0.1484\n",
            "Epoch 16/20\n",
            "5/5 [==============================] - 2s 339ms/step - loss: 3.3042 - accuracy: 0.1563\n",
            "Epoch 17/20\n",
            "5/5 [==============================] - 2s 337ms/step - loss: 3.2884 - accuracy: 0.1516\n",
            "Epoch 18/20\n",
            "5/5 [==============================] - 2s 339ms/step - loss: 3.2612 - accuracy: 0.1647\n",
            "Epoch 19/20\n",
            "5/5 [==============================] - 2s 340ms/step - loss: 3.2231 - accuracy: 0.1811\n",
            "Epoch 20/20\n",
            "5/5 [==============================] - 2s 339ms/step - loss: 3.1707 - accuracy: 0.1935\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ec0f805ad40>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next(sample: str, model: keras.Sequential, tokenizer: Dict[str, int], vocabulary: Dict[int, str], n_next: int, temperature: float, batch_size: int, word: bool = False) -> str:\n",
        "    if word:\n",
        "        sample_vector = [tokenizer[word] for word in sample.split()]\n",
        "    else:\n",
        "        sample_vector = [tokenizer[char] for char in sample]\n",
        "    predicted = sample_vector\n",
        "    sample_tensor = tf.expand_dims(sample_vector, 0)\n",
        "    sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    for i in range(n_next):\n",
        "        pred = model(sample_tensor)\n",
        "        pred = pred[0].numpy() / temperature\n",
        "        pred = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
        "        predicted.append(pred)\n",
        "        sample_tensor = predicted[-99:]\n",
        "        sample_tensor = tf.expand_dims([pred], 0)\n",
        "        sample_tensor = tf.repeat(sample_tensor, batch_size, axis=0)\n",
        "    pred_seq = [vocabulary[i] for i in predicted]\n",
        "    generated = ' '.join(pred_seq) if word else ''.join(pred_seq)\n",
        "    return generated"
      ],
      "metadata": {
        "id": "qPRCuF4jtcrQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Гарри',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=200,\n",
        "    temperature=0.6,\n",
        "    batch_size=BATCH_SIZE\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mif8PcHiGzv3",
        "outputId": "f774d22b-9500-459d-d05b-4938ff716e77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Гарри а о и  а е зо двл мо ечдоноло ч а о и лоепо и  и е,  ло оне\n",
            "нс та натланетьеро ре а сагЕни аалунне ла о ннете ско ирмуы у о о и ,ливирилра о а а о ате\n",
            "а логн дноле и р, у зотми риль о ино о не мне ии\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='Магия',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=100,\n",
        "    temperature=0.2,\n",
        "    batch_size=BATCH_SIZE\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4BA_0irxMui",
        "outputId": "761d07ab-d1d1-465f-c54a-25b4975e6e2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Магия а о а но а и но о  о а на а а  а о и е и  о о  а а  о е а и  и а о  о а и и  о  но о  о ро а  о о а\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next(\n",
        "    sample='МакГонагалл',\n",
        "    model=model,\n",
        "    tokenizer=word_index,\n",
        "    vocabulary=index_word,\n",
        "    n_next=150,\n",
        "    temperature=0.81,\n",
        "    batch_size=BATCH_SIZE\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nbrkLYzxRPV",
        "outputId": "d54b665f-ede6-4406-98cb-77832fb86ae5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "МакГонагалленила з ини я, угискь кери сакаланивнии ора ро о лс к рнт—азигялрту — ноо енино ь-.генмнжрло сое ори е Гажьремосооана а а \n",
            "о оо\n",
            "ов \n",
            "ащва бку бе к вмаы\n"
          ]
        }
      ]
    }
  ]
}